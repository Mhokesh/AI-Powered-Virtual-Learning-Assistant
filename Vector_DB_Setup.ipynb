{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 15:34:26,817 [INFO] Use pytorch device_name: cuda\n",
      "2025-02-28 15:34:26,818 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-02-28 15:34:32,728 [INFO] HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\MATHAN\\AppData\\Local\\Temp\\ipykernel_21560\\3829270918.py:9: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "2025-02-28 15:34:32,748 [INFO] HTTP Request: DELETE http://localhost:6333/collections/Olabs_final \"HTTP/1.1 200 OK\"\n",
      "2025-02-28 15:34:33,192 [INFO] HTTP Request: PUT http://localhost:6333/collections/Olabs_final \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the embedding model and Qdrant client\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vector_dim = model.get_sentence_embedding_dimension()\n",
    "client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "collection_name = \"Olabs_books\"\n",
    "\n",
    "# Recreate the Qdrant collection with required configuration\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_dim, distance=\"Cosine\")\n",
    ")\n",
    "\n",
    "# Define regex patterns and sets for metadata extraction\n",
    "class_pattern = re.compile(r\"Class\\s*\\d+\")\n",
    "doc_types = {\"activities\", \"Lab Manual\", \"Projects\",}\n",
    "subject_keywords = {\"science\", \"maths\", \"biology\", \"chemistry\", \"physics\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract text from a PDF using pdfminer.six and remove excessive whitespace.\"\"\"\n",
    "    try:\n",
    "        text = pdfminer_extract_text(str(pdf_path))\n",
    "        if text:\n",
    "            return \" \".join(text.split())\n",
    "        else:\n",
    "            logging.warning(f\"No text found in {pdf_path}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Split text into chunks.\n",
    "    Default chunk_size is 200 words with an overlap of 50 words.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def extract_metadata(file_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from the file path based on folder names.\n",
    "    Expected metadata includes 'class', 'doc_type', and 'subject'.\n",
    "    \"\"\"\n",
    "    metadata = {\"class\": None, \"doc_type\": None, \"subject\": None}\n",
    "    for part in file_path.parts:\n",
    "        if metadata[\"class\"] is None:\n",
    "            m = class_pattern.search(part)\n",
    "            if m:\n",
    "                metadata[\"class\"] = m.group().strip()\n",
    "        if metadata[\"doc_type\"] is None and part.lower() in {d.lower() for d in doc_types}:\n",
    "            metadata[\"doc_type\"] = part\n",
    "        if metadata[\"subject\"] is None:\n",
    "            for keyword in subject_keywords:\n",
    "                if keyword in part.lower():\n",
    "                    metadata[\"subject\"] = part\n",
    "                    break\n",
    "    return metadata\n",
    "\n",
    "def process_pdf(pdf_file: Path, id_num: int, chunk_size: int = 200, overlap: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Process a PDF file by extracting its text, chunking into 200-word segments,\n",
    "    generating embeddings for each chunk, and attaching metadata.\n",
    "    Returns a list of Qdrant PointStruct objects.\n",
    "    \"\"\"\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    if not text:\n",
    "        logging.warning(f\"Skipping {pdf_file} due to no extractable text.\")\n",
    "        return []\n",
    "    \n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "    metadata = extract_metadata(pdf_file)\n",
    "    metadata[\"file_name\"] = pdf_file.name\n",
    "    metadata[\"pdf_path\"] = str(pdf_file)\n",
    "    \n",
    "    points = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        embedding = model.encode(chunk).tolist()\n",
    "        payload = metadata.copy()\n",
    "        payload[\"chunk_index\"] = idx\n",
    "        payload[\"chunk_text\"] = chunk  # store the full chunk text\n",
    "        point = PointStruct(\n",
    "            id=id_num,\n",
    "            vector=embedding,\n",
    "            payload=payload\n",
    "        )\n",
    "        points.append(point)\n",
    "        id_num += 1\n",
    "    return points\n",
    "\n",
    "def process_dataset(root_dir: Path, max_workers: int = 4, batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Traverse the dataset directory, process PDFs concurrently,\n",
    "    upsert the resulting points into Qdrant in batches, and log progress.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "    id_counter = 1\n",
    "\n",
    "    pdf_files = list(root_dir.rglob(\"*.pdf\"))\n",
    "    total_files = len(pdf_files)\n",
    "    logging.info(f\"Found {total_files} PDF files to process.\")\n",
    "\n",
    "    processed_count = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_pdf, pdf_file, id_counter + idx): pdf_file\n",
    "            for idx, pdf_file in enumerate(pdf_files)\n",
    "        }\n",
    "        for future in as_completed(futures):\n",
    "            processed_count += 1\n",
    "            points = future.result()\n",
    "            if points:\n",
    "                all_points.extend(points)\n",
    "            else:\n",
    "                logging.info(f\"File skipped: {futures[future]}\")\n",
    "            logging.info(f\"Processed {processed_count} of {total_files} PDFs.\")\n",
    "\n",
    "    logging.info(f\"Successfully processed {len(all_points)} chunks from PDFs.\")\n",
    "\n",
    "    # Upsert points into Qdrant in batches\n",
    "    for i in range(0, len(all_points), batch_size):\n",
    "        batch = all_points[i:i+batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch)\n",
    "        logging.info(f\"Upserted batch {(i // batch_size) + 1} with {len(batch)} points.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your dataset root directory\n",
    "    dataset_root = Path(r\"N:\\CAI\\hackathon\\olabs\\dataset\")\n",
    "    process_dataset(dataset_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
